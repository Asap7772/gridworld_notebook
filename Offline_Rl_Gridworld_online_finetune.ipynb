{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi6tzGm083lm"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Setup Block # 1:\n",
    "    SZ: edited Dec. 29, re-arranged\n",
    "    This block contains definitions\n",
    "\"\"\"\n",
    "\n",
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.patches import Rectangle, Polygon\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import scipy\n",
    "%matplotlib inline  \n",
    "\n",
    "#@title Environment Code\n",
    "import sys\n",
    "\n",
    "EMPTY = 110\n",
    "WALL = 111\n",
    "START = 112\n",
    "REWARD = 113\n",
    "OUT_OF_BOUNDS = 114\n",
    "LAVA = 118\n",
    "\n",
    "TILES = {EMPTY, WALL, START, REWARD, LAVA}\n",
    "\n",
    "STR_MAP = {\n",
    "    'O': EMPTY,\n",
    "    '#': WALL,\n",
    "    'S': START,\n",
    "    'R': REWARD,\n",
    "    'L': LAVA\n",
    "}\n",
    "\n",
    "RENDER_DICT = {v:k for k, v in STR_MAP.items()}\n",
    "RENDER_DICT[EMPTY] = ' '\n",
    "RENDER_DICT[START] = ' '\n",
    "\n",
    "\n",
    "def spec_from_string(s, valmap=STR_MAP):\n",
    "    if s.endswith('\\\\'):\n",
    "        s = s[:-1]\n",
    "    rows = s.split('\\\\')\n",
    "    rowlens = np.array([len(row) for row in rows])\n",
    "    assert np.all(rowlens == rowlens[0])\n",
    "    w, h = len(rows[0]), len(rows)\n",
    "\n",
    "    gs = GridSpec(w, h)\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            gs[j,i] = valmap[rows[i][j]]\n",
    "    return gs\n",
    "\n",
    "\n",
    "def spec_from_sparse_locations(w, h, tile_to_locs):\n",
    "    \"\"\"\n",
    "\n",
    "    Example usage:\n",
    "    >> spec_from_sparse_locations(10, 10, {START: [(0,0)], REWARD: [(7,8), (8,8)]})\n",
    "\n",
    "    \"\"\"\n",
    "    gs = GridSpec(w, h)\n",
    "    for tile_type in tile_to_locs:\n",
    "        locs = np.array(tile_to_locs[tile_type])\n",
    "        for i in range(locs.shape[0]):\n",
    "            gs[tuple(locs[i])] = tile_type\n",
    "    return gs\n",
    "\n",
    "\n",
    "def local_spec(map, xpnt):\n",
    "    \"\"\"\n",
    "    >>> local_spec(\"yOy\\\\\\\\Oxy\", xpnt=(5,5))\n",
    "    array([[4, 4],\n",
    "           [6, 4],\n",
    "           [6, 5]])\n",
    "    \"\"\"\n",
    "    Y = 0; X=1; O=2\n",
    "    valmap={\n",
    "        'y': Y,\n",
    "        'x': X,\n",
    "        'O': O\n",
    "    }\n",
    "    gs = spec_from_string(map, valmap=valmap)\n",
    "    ys = gs.find(Y)\n",
    "    x = gs.find(X)\n",
    "    result = ys-x + np.array(xpnt)\n",
    "    return result\n",
    "\n",
    "\n",
    "class GridSpec(object):\n",
    "    def __init__(self, w, h):\n",
    "        self.__data = np.zeros((w, h), dtype=np.int32)\n",
    "        self.__w = w\n",
    "        self.__h = h\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self.__data[key] = val\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.out_of_bounds(key):\n",
    "            raise NotImplementedError(\"Out of bounds:\"+str(key))\n",
    "        return self.__data[tuple(key)]\n",
    "\n",
    "    def out_of_bounds(self, wh):\n",
    "        \"\"\" Return true if x, y is out of bounds \"\"\"\n",
    "        w, h = wh\n",
    "        if w<0 or w>=self.__w:\n",
    "            return True\n",
    "        if h < 0 or h >= self.__h:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_neighbors(self, k, xy=False):\n",
    "        \"\"\" Return values of up, down, left, and right tiles \"\"\"\n",
    "        if not xy:\n",
    "            k = self.idx_to_xy(k)\n",
    "        offsets = [np.array([0,-1]), np.array([0,1]),\n",
    "                   np.array([-1,0]), np.array([1,0])]\n",
    "        neighbors = \\\n",
    "            [self[k+offset] if (not self.out_of_bounds(k+offset)) else OUT_OF_BOUNDS for offset in offsets ]\n",
    "        return neighbors\n",
    "\n",
    "    def get_value(self, k, xy=False):\n",
    "        \"\"\" Return values of up, down, left, and right tiles \"\"\"\n",
    "        if not xy:\n",
    "            k = self.idx_to_xy(k)\n",
    "        return self[k]\n",
    "\n",
    "    def find(self, value):\n",
    "        return np.array(np.where(self.spec == value)).T\n",
    "\n",
    "    @property\n",
    "    def spec(self):\n",
    "        return self.__data\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return self.__w\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.__w*self.__h\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return self.__h\n",
    "\n",
    "    def idx_to_xy(self, idx):\n",
    "        if hasattr(idx, '__len__'):  # array\n",
    "            x = idx % self.__w\n",
    "            y = np.floor(idx/self.__w).astype(np.int32)\n",
    "            xy = np.c_[x,y]\n",
    "            return xy\n",
    "        else:\n",
    "            return np.array([ idx % self.__w, int(np.floor(idx/self.__w))])\n",
    "\n",
    "    def xy_to_idx(self, key):\n",
    "        shape = np.array(key).shape\n",
    "        if len(shape) == 1:\n",
    "            return key[0] + key[1]*self.__w\n",
    "        elif len(shape) == 2:\n",
    "            return key[:,0] + key[:,1]*self.__w\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "    def __hash__(self):\n",
    "        data = (self.__w, self.__h) + tuple(self.__data.reshape([-1]).tolist())\n",
    "        return hash(data)\n",
    " \n",
    "\n",
    "ACT_NOOP = 0\n",
    "ACT_UP = 1\n",
    "ACT_DOWN = 2\n",
    "ACT_LEFT = 3\n",
    "ACT_RIGHT = 4\n",
    "ACT_DICT = {\n",
    "    ACT_NOOP: [0,0],\n",
    "    ACT_UP: [0, -1],\n",
    "    ACT_LEFT: [-1, 0],\n",
    "    ACT_RIGHT: [+1, 0],\n",
    "    ACT_DOWN: [0, +1]\n",
    "}\n",
    "ACT_TO_STR = {\n",
    "    ACT_NOOP: 'NOOP',\n",
    "    ACT_UP: 'UP',\n",
    "    ACT_LEFT: 'LEFT',\n",
    "    ACT_RIGHT: 'RIGHT',\n",
    "    ACT_DOWN: 'DOWN'\n",
    "}\n",
    "\n",
    "class TransitionModel(object):\n",
    "    def __init__(self, gridspec, eps=0.2):\n",
    "        self.gs = gridspec\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_aprobs(self, s, a):\n",
    "        # TODO: could probably output a matrix over all states...\n",
    "        legal_moves = self.__get_legal_moves(s)\n",
    "        p = np.zeros(len(ACT_DICT))\n",
    "        p[legal_moves] = self.eps / (len(legal_moves))\n",
    "        if a in legal_moves:\n",
    "            p[a] += 1.0-self.eps\n",
    "        else:\n",
    "            #p = np.array([1.0,0,0,0,0])  # NOOP\n",
    "            p[ACT_NOOP] += 1.0-self.eps\n",
    "        return p\n",
    "\n",
    "    def __get_legal_moves(self, s):\n",
    "        xy = np.array(self.gs.idx_to_xy(s))\n",
    "        moves = [move for move in ACT_DICT if not self.gs.out_of_bounds(xy+ACT_DICT[move])\n",
    "                                             and self.gs[xy+ACT_DICT[move]] != WALL]\n",
    "        return moves\n",
    "\n",
    "      \n",
    "OBS_ONEHOT = 'onehot'\n",
    "OBS_RANDOM = 'random'\n",
    "OBS_SMOOTH = 'smooth'\n",
    "\n",
    "\n",
    "class GridEnv(object):\n",
    "    def __init__(self, gridspec, \n",
    "                 teps=0.0,\n",
    "                 observation_type=OBS_ONEHOT,\n",
    "                 dim_obs=8):\n",
    "        super(GridEnv, self).__init__()\n",
    "        self.num_states = len(gridspec)\n",
    "        self.num_actions = 5\n",
    "        self.obs_type = observation_type\n",
    "        self.gs = gridspec\n",
    "        self.model = TransitionModel(gridspec, eps=teps)\n",
    "        self._transition_matrix = None\n",
    "        self._transition_matrix = self.transition_matrix()\n",
    "\n",
    "        if self.obs_type == OBS_RANDOM:\n",
    "          self.dim_obs = dim_obs\n",
    "          self.obs_matrix = np.random.randn(self.num_states, self.dim_obs)\n",
    "        elif self.obs_type == OBS_SMOOTH:\n",
    "          self.dim_obs = dim_obs\n",
    "          self.obs_matrix = np.random.randn(self.num_states, self.dim_obs)\n",
    "          trans_matrix = np.sum(self._transition_matrix, axis=1) / self.num_actions\n",
    "          for k in range(10):\n",
    "            cur_obs_mat = self.obs_matrix[:,:]\n",
    "            for state in range(self.num_states):\n",
    "                new_obs = trans_matrix[state].dot(cur_obs_mat)\n",
    "                self.obs_matrix[state] = new_obs\n",
    "        else:\n",
    "          self.dim_obs = self.gs.width+self.gs.height\n",
    "\n",
    "    def observation(self, s):\n",
    "        if self.obs_type == OBS_ONEHOT:\n",
    "          xy_vec = np.zeros(self.gs.width+self.gs.height)\n",
    "          xy = self.gs.idx_to_xy(s)\n",
    "          xy_vec[xy[0]] = 1.0\n",
    "          xy_vec[xy[1]+self.gs.width] = 1.0\n",
    "          return xy_vec\n",
    "        elif self.obs_type == OBS_RANDOM or self.obs_type == OBS_SMOOTH:\n",
    "          return self.obs_matrix[s]\n",
    "        else:\n",
    "          raise ValueError(\"Invalid obs type %s\" % self.obs_type)\n",
    "        \n",
    "    def reward(self, s, a, ns):\n",
    "        \"\"\" \n",
    "        Returns the reward (float)\n",
    "        \"\"\"\n",
    "        tile_type = self.gs[self.gs.idx_to_xy(s)]\n",
    "        if tile_type == REWARD:\n",
    "          return 1\n",
    "        elif tile_type == LAVA:\n",
    "          return -1\n",
    "        else:\n",
    "          return 0\n",
    "\n",
    "    def transitions(self, s, a):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of next_state (int) -> prob (float)\n",
    "        \"\"\"\n",
    "        tile_type = self.gs[self.gs.idx_to_xy(s)]\n",
    "        #if tile_type == LAVA: # Lava gets you stuck\n",
    "        #    return {s: 1.0}\n",
    "        if tile_type == WALL:\n",
    "          return {s: 1.0}\n",
    "\n",
    "        aprobs = self.model.get_aprobs(s, a)\n",
    "        t_dict = {}\n",
    "        for sa in range(5):\n",
    "            if aprobs[sa] > 0:\n",
    "                next_s = self.gs.idx_to_xy(s) + ACT_DICT[sa]\n",
    "                next_s_idx = self.gs.xy_to_idx(next_s)\n",
    "                t_dict[next_s_idx] = t_dict.get(next_s_idx, 0.0) + aprobs[sa]\n",
    "        return t_dict\n",
    "      \n",
    "    def initial_state_distribution(self):\n",
    "        start_idxs = np.array(np.where(self.gs.spec == START)).T\n",
    "        num_starts = start_idxs.shape[0]\n",
    "        initial_distribution = {}\n",
    "        for i in range(num_starts):\n",
    "          initial_distribution[self.gs.xy_to_idx(start_idxs[i])] = 1.0/num_starts \n",
    "        return initial_distribution\n",
    "\n",
    "    def step_stateless(self, s, a, verbose=False):\n",
    "        probs = list(self.transitions(s, a).items())\n",
    "        ns_idx = np.random.choice(range(len(probs)), p=[p[1] for p in probs])\n",
    "        ns = probs[ns_idx][0]\n",
    "        rew = self.reward(s, a, ns)\n",
    "        return ns, rew\n",
    "\n",
    "    def step(self, a, verbose=False):\n",
    "        ns, r = self.step_stateless(self.__state, a, verbose=verbose)\n",
    "        self.__state = ns\n",
    "        return ns, r, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        init_distr = list(self.initial_state_distribution().items())\n",
    "        start_idx = np.random.choice(len(init_distr), p=[p[1] for p in init_distr])\n",
    "        self.__state = init_distr[start_idx][0]\n",
    "        self._timestep = 0\n",
    "        return start_idx\n",
    "\n",
    "    def render(self, close=False, ostream=sys.stdout):\n",
    "        if close:\n",
    "            return\n",
    "\n",
    "        state = self.__state\n",
    "        ostream.write('-'*(self.gs.width+2)+'\\n')\n",
    "        for h in range(self.gs.height):\n",
    "            ostream.write('|')\n",
    "            for w in range(self.gs.width):\n",
    "                if self.gs.xy_to_idx((w,h)) == state:\n",
    "                    ostream.write('*')\n",
    "                else:\n",
    "                    val = self.gs[w, h]\n",
    "                    ostream.write(RENDER_DICT[val])\n",
    "            ostream.write('|\\n')\n",
    "        ostream.write('-' * (self.gs.width + 2)+'\\n')\n",
    "        \n",
    "    def transition_matrix(self):\n",
    "        if self._transition_matrix is None:\n",
    "          transition_matrix = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
    "          for s in range(self.num_states):\n",
    "            for a in range(self.num_actions):\n",
    "              for ns, prob in self.transitions(s, a).items():\n",
    "                transition_matrix[s,a,ns] = prob\n",
    "          self._transition_matrix = transition_matrix\n",
    "        return self._transition_matrix\n",
    "\n",
    "def evaluate(policy, s):\n",
    "    action_dist = policy[s]\n",
    "    return np.random.choice(range(len(action_dist)), p=action_dist)\n",
    "\n",
    "def rollout(env, policy, T, render=False):\n",
    "    dS = env.num_states\n",
    "    dA = env.num_actions\n",
    "    transitions=[]\n",
    "    s = env.reset()\n",
    "    \n",
    "    assert isinstance(policy, torch.Tensor) or isinstance(policy, np.ndarray), f\"policy must be a torch.Tensor or np.ndarray, got {type(policy)}\"\n",
    "    assert policy.shape == (dS, dA)\n",
    "    if isinstance(policy, torch.Tensor):\n",
    "      policy = policy.detach().numpy()\n",
    "      \n",
    "    for i in range(T):\n",
    "      a = evaluate(policy, s)\n",
    "      ns, r, done, info = env.step(a)\n",
    "      transitions.append((s, a, r, ns))\n",
    "      s = ns\n",
    "    \n",
    "    return transitions\n",
    "  \n",
    "def rollout_trajs(env, policy, T, num_trajs, render=False):\n",
    "    all_trajs = []\n",
    "    for _ in range(num_trajs):\n",
    "      all_trajs.extend(rollout(env, policy, T, render=render))\n",
    "    return all_trajs\n",
    "    \n",
    "#@title Plotting Code\n",
    "PLT_NOOP = np.array([[-0.1,0.1], [-0.1,-0.1], [0.1,-0.1], [0.1,0.1]])\n",
    "PLT_UP = np.array([[0,0], [0.5,0.5], [-0.5,0.5]])\n",
    "PLT_LEFT = np.array([[0,0], [-0.5,0.5], [-0.5,-0.5]])\n",
    "PLT_RIGHT = np.array([[0,0], [0.5,0.5], [0.5,-0.5]])\n",
    "PLT_DOWN = np.array([[0,0], [0.5,-0.5], [-0.5,-0.5]])\n",
    "\n",
    "TXT_OFFSET_VAL = 0.3\n",
    "TXT_CENTERING = np.array([-0.08, -0.05])\n",
    "TXT_NOOP = np.array([0.0,0])+TXT_CENTERING\n",
    "TXT_UP = np.array([0,TXT_OFFSET_VAL])+TXT_CENTERING\n",
    "TXT_LEFT = np.array([-TXT_OFFSET_VAL,0])+TXT_CENTERING\n",
    "TXT_RIGHT = np.array([TXT_OFFSET_VAL,0])+TXT_CENTERING\n",
    "TXT_DOWN = np.array([0,-TXT_OFFSET_VAL])+TXT_CENTERING\n",
    "\n",
    "ACT_OFFSETS = [\n",
    "    [PLT_NOOP, TXT_NOOP],\n",
    "    [PLT_UP, TXT_UP],\n",
    "    [PLT_DOWN, TXT_DOWN],\n",
    "    [PLT_LEFT, TXT_LEFT],\n",
    "    [PLT_RIGHT, TXT_RIGHT]\n",
    "]\n",
    "\n",
    "PLOT_CMAP = cm.RdYlBu\n",
    "\n",
    "def plot_sa_values(env, q_values, text_values=False, \n",
    "                   invert_y=True, update=False,\n",
    "                   title=None):\n",
    "  w = env.gs.width\n",
    "  h = env.gs.height\n",
    "  \n",
    "  if update:\n",
    "    clear_output(wait=True)\n",
    "  plt.figure(figsize=(2*w, 2*h))\n",
    "  ax = plt.gca()\n",
    "  normalized_values = q_values\n",
    "  normalized_values = normalized_values - np.min(normalized_values)\n",
    "  normalized_values = normalized_values/np.max(normalized_values)\n",
    "  for x, y in itertools.product(range(w), range(h)):\n",
    "      state_idx = env.gs.xy_to_idx((x, y))\n",
    "      if invert_y:\n",
    "          y = h-y-1\n",
    "      xy = np.array([x, y])\n",
    "      xy3 = np.expand_dims(xy, axis=0)\n",
    "\n",
    "      for a in range(4, -1, -1):\n",
    "          val = normalized_values[state_idx,a]\n",
    "          og_val = q_values[state_idx,a]\n",
    "          patch_offset, txt_offset = ACT_OFFSETS[a]\n",
    "          if text_values:\n",
    "              xy_text = xy+txt_offset\n",
    "              ax.text(xy_text[0], xy_text[1], '%.2f'%og_val, size='small')\n",
    "          color = PLOT_CMAP(val)\n",
    "          ax.add_patch(Polygon(xy3+patch_offset, True,\n",
    "                                     color=color))\n",
    "  ax.set_xticks(np.arange(-1, w+1, 1))\n",
    "  ax.set_yticks(np.arange(-1, h+1, 1))\n",
    "  plt.grid()\n",
    "  if title:\n",
    "    # SZ Dec. 28: added labelsize\n",
    "    plt.title(title, fontdict={'fontsize': 36})\n",
    "  plt.show()\n",
    "\n",
    "def plot_s_values(env, v_values, text_values=True, \n",
    "                  invert_y=True, update=False,\n",
    "                  title=None):\n",
    "  w = env.gs.width\n",
    "  h = env.gs.height\n",
    "  if update:\n",
    "    clear_output(wait=True)\n",
    "  plt.figure(figsize=(2*w, 2*h))\n",
    "  ax = plt.gca()\n",
    "  normalized_values = v_values\n",
    "  normalized_values = normalized_values - np.min(normalized_values)\n",
    "  normalized_values = normalized_values/np.max(normalized_values)\n",
    "  for x, y in itertools.product(range(w), range(h)):\n",
    "      state_idx = env.gs.xy_to_idx((x, y))\n",
    "      if invert_y:\n",
    "          y = h-y-1\n",
    "      xy = np.array([x, y])\n",
    "\n",
    "      val = normalized_values[state_idx]\n",
    "      og_val = v_values[state_idx]\n",
    "      if text_values:\n",
    "          xy_text = xy\n",
    "          ax.text(xy_text[0], xy_text[1], '%.2f'%og_val, size='small')\n",
    "      color = PLOT_CMAP(val)\n",
    "      ax.add_patch(Rectangle(xy-0.5, 1, 1, color=color))\n",
    "  ax.set_xticks(np.arange(-1, w+1, 1))\n",
    "  ax.set_yticks(np.arange(-1, h+1, 1))\n",
    "  plt.grid()  \n",
    "  if title:\n",
    "    plt.title(title)\n",
    "  plt.show()\n",
    "  \n",
    "#@title Neural Network Code\n",
    "\n",
    "def stack_observations(env):\n",
    "    obs = []\n",
    "    for s in range(env.num_states):\n",
    "        obs.append(env.observation(s))\n",
    "    return np.stack(obs)\n",
    "\n",
    "class FCNetwork(torch.nn.Module):\n",
    "  def __init__(self, env, layers=[20,20]):\n",
    "    super(FCNetwork, self).__init__()\n",
    "    self.all_observations = torch.tensor(stack_observations(env), dtype=torch.float32)\n",
    "    dim_input = env.dim_obs\n",
    "    dim_output = env.num_actions\n",
    "    net_layers = []\n",
    "\n",
    "    dim = dim_input\n",
    "    for i, layer_size in enumerate(layers):\n",
    "      net_layers.append(torch.nn.Linear(dim, layer_size))\n",
    "      net_layers.append(torch.nn.ReLU())\n",
    "      dim = layer_size\n",
    "    net_layers.append(torch.nn.Linear(dim, dim_output))\n",
    "    self.layers = net_layers\n",
    "    self.network = torch.nn.Sequential(*net_layers)\n",
    "\n",
    "  def forward(self, states):\n",
    "    observations = torch.index_select(self.all_observations, 0, states) \n",
    "    return self.network(observations)\n",
    "  \n",
    "\n",
    "def one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return y_one_hot\n",
    "  \n",
    "\n",
    "class TabularNetwork(torch.nn.Module):\n",
    "  def __init__(self, env):\n",
    "    super(TabularNetwork, self).__init__()\n",
    "    self.num_states = env.num_states\n",
    "    self.network = torch.nn.Sequential(\n",
    "        torch.nn.Linear(self.num_states, env.num_actions)\n",
    "    )\n",
    "\n",
    "  def forward(self, states):\n",
    "    onehot = one_hot(states, self.num_states)\n",
    "    return self.network(onehot)\n",
    "\n",
    "#@title Utility Functions\n",
    "\n",
    "def compute_policy_deterministic(q_values, eps_greedy=0.0):\n",
    "  policy_probs = np.zeros_like(q_values)\n",
    "  policy_probs[np.arange(policy_probs.shape[0]), np.argmax(q_values, axis=1)] = 1.0 - eps_greedy\n",
    "  policy_probs += eps_greedy / (policy_probs.shape[1])\n",
    "  return policy_probs\n",
    "\n",
    "def compute_policy_stochastic(q_values, tau=1.0):\n",
    "  q_values = q_values / tau\n",
    "  \n",
    "  if isinstance(q_values, torch.Tensor):\n",
    "    softmax_fn = torch.nn.Softmax(dim=1)\n",
    "  elif isinstance(q_values, np.ndarray):\n",
    "    softmax_fn = scipy.special.softmax\n",
    "  \n",
    "  return softmax_fn(q_values)\n",
    "\n",
    "def compute_visitation(env, policy, discount=1.0, T=50):\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "  state_visitation = np.zeros((dS, 1))\n",
    "  for (state, prob) in env.initial_state_distribution().items():\n",
    "    state_visitation[state] = prob\n",
    "  t_matrix = env.transition_matrix()  # S x A x S\n",
    "  sa_visit_t = np.zeros((dS, dA, T))\n",
    "\n",
    "  norm_factor = 0.0\n",
    "  for i in range(T):\n",
    "    sa_visit = state_visitation * policy\n",
    "    cur_discount = (discount ** i)\n",
    "    sa_visit_t[:, :, i] = cur_discount * sa_visit\n",
    "    norm_factor += cur_discount\n",
    "    # sum-out (SA)S\n",
    "    new_state_visitation = np.einsum('ij,ijk->k', sa_visit, t_matrix)\n",
    "    state_visitation = np.expand_dims(new_state_visitation, axis=1)\n",
    "  return np.sum(sa_visit_t, axis=2) / norm_factor\n",
    "\n",
    "\"\"\"\n",
    "  Calculating KL between dist \\pi_1(a|s) and dist \\pi_2(a|s) of dimension S x A\n",
    "  Distributions can be unnormalized (e.g exp(Q) is a valid distribution)\n",
    "  KL calculated as E_{s \\in S, a \\sim \\pi_1(a|s)}[log(\\pi_1(a|s)/\\pi_2(a|s))]\n",
    "\"\"\"\n",
    "def compute_divergence(dist1, dist2):\n",
    "  norm_dist1 = dist1/(dist1.sum(-1, keepdims=True) + 1e-9)\n",
    "  norm_dist2 = dist2/(dist2.sum(-1, keepdims=True) + 1e-9)\n",
    "  summ = (norm_dist1 * np.log(norm_dist1/(norm_dist2 + 1e-9) + 1e-9)).sum(-1) # should be S\n",
    "  return summ.mean()\n",
    "  \n",
    "\n",
    "def compute_return(env, policy, discount=1.0, T=50):\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "  state_visitation = np.zeros((dS, 1))\n",
    "\n",
    "  for (state, prob) in env.initial_state_distribution().items():\n",
    "    state_visitation[state] = prob\n",
    "  \n",
    "  rewards = np.zeros((dS, 1))\n",
    "  w = env.gs.width\n",
    "  h = env.gs.height\n",
    "  for x, y in itertools.product(range(w), range(h)):\n",
    "    state = env.gs.xy_to_idx((x, y))\n",
    "    state_xy = env.gs.idx_to_xy(state)\n",
    "    tile = env.gs[state_xy]\n",
    "    rewards[state] = -1 if tile == LAVA else rewards[state]\n",
    "    rewards[state] = 1 if tile == REWARD else rewards[state]\n",
    "  t_matrix = env.transition_matrix()  # S x A x S\n",
    "  sa_visit_t = np.zeros((dS, dA, T))\n",
    "\n",
    "  norm_factor = 0.0\n",
    "  for i in range(T):\n",
    "    sa_visit = state_visitation * policy\n",
    "    cur_discount = (discount ** i)\n",
    "    sa_visit_t[:, :, i] = cur_discount * sa_visit\n",
    "    norm_factor += cur_discount\n",
    "    # sum-out (SA)S\n",
    "    new_state_visitation = np.einsum('ij,ijk->k', sa_visit, t_matrix)\n",
    "    state_visitation = np.expand_dims(new_state_visitation, axis=1)\n",
    "  \n",
    "  vis = np.sum(sa_visit_t, axis=2) / norm_factor\n",
    "  return (vis*rewards).sum() \n",
    "\n",
    "def get_tensors(list_of_tensors, list_of_indices):\n",
    "  s, a, ns, r = [], [], [], []\n",
    "  for idx in list_of_indices:\n",
    "    s.append(list_of_tensors[idx][0])\n",
    "    a.append(list_of_tensors[idx][1])\n",
    "    r.append(list_of_tensors[idx][2])\n",
    "    ns.append(list_of_tensors[idx][3])\n",
    "  s = np.array(s)\n",
    "  a = np.array(a)\n",
    "  ns = np.array(ns)\n",
    "  r = np.array(r)\n",
    "  return s, a, ns, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "duwzW6oe83lz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "    Setup Block # 2:\n",
    "    SZ: edited Dec. 29, re-arranged\n",
    "    This block contains different Q function updates\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#@title Tabular Q-iteration\n",
    "\n",
    "def q_backup_sparse(env, q_values, discount=0.99):\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "    \n",
    "  new_q_values = np.zeros_like(q_values)\n",
    "  value = np.max(q_values, axis=1)\n",
    "  for s in range(dS):\n",
    "    for a in range(dA):\n",
    "      new_q_value = 0\n",
    "      for ns, prob in env.transitions(s, a).items():\n",
    "        new_q_value += prob * (env.reward(s,a,ns) + discount*value[ns])\n",
    "      new_q_values[s,a] = new_q_value\n",
    "  return new_q_values\n",
    "\n",
    "def q_backup_sparse_sampled(env, q_values, s, a, ns, r, discount=0.99):\n",
    "  q_values_ns = q_values[ns, :]\n",
    "  values = np.max(q_values_ns, axis=-1)\n",
    "  target_value = r + discount * values\n",
    "  return target_value\n",
    "\n",
    "def q_backup_sparse_policy(env, q_values, policy, discount=0.99):\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "    \n",
    "  new_q_values = np.zeros_like(q_values)\n",
    "  value = np.sum(q_values * policy, axis=1)\n",
    "  for s in range(dS):\n",
    "    for a in range(dA):\n",
    "      new_q_value = 0\n",
    "      for ns, prob in env.transitions(s, a).items():\n",
    "        new_q_value += prob * (env.reward(s,a,ns) + discount*value[ns])\n",
    "      new_q_values[s,a] = new_q_value\n",
    "  return new_q_values\n",
    "\n",
    "def q_backup_sparse_sampled_policy(env, q_values, s, a, ns, r, policy, discount=0.99):\n",
    "  q_values_ns = q_values[ns, :]\n",
    "  policy_ns = policy[ns, :]\n",
    "  values = np.sum(q_values_ns * policy_ns, axis=-1)\n",
    "  target_value = r + discount * values\n",
    "  return target_value\n",
    "\n",
    "def q_iteration(env, num_itrs=100, render=False, **kwargs):\n",
    "  \"\"\"\n",
    "  Run tabular Q-iteration\n",
    "  \n",
    "  Args:\n",
    "    env: A GridEnv object\n",
    "    num_itrs (int): Number of FQI iterations to run\n",
    "    render (bool): If True, will plot q-values after each iteration\n",
    "  \"\"\"\n",
    "  q_values = np.zeros((env.num_states, env.num_actions))\n",
    "  for i in range(num_itrs):\n",
    "    q_values = q_backup_sparse(env, q_values, **kwargs)\n",
    "    if render:\n",
    "      plot_sa_values(env, q_values, update=True, title='Q-values')\n",
    "  return q_values\n",
    "\n",
    "#@title Fitted Q-iteration\n",
    "\n",
    "def project_qvalues(q_values, network, optimizer, num_steps=50, weights=None):\n",
    "    # regress onto q_values (aka projection)\n",
    "    q_values_tensor = torch.tensor(q_values, dtype=torch.float32)\n",
    "    for _ in range(num_steps):\n",
    "       # Eval the network at each state\n",
    "      pred_qvalues = network(torch.arange(q_values.shape[0]))\n",
    "      if weights is None:\n",
    "        loss = torch.mean((pred_qvalues - q_values_tensor)**2)\n",
    "      else:\n",
    "        loss = torch.mean(weights*(pred_qvalues - q_values_tensor)**2)\n",
    "      network.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    return pred_qvalues.detach().numpy()\n",
    "\n",
    "def project_qvalues_sampled(env, s, a, target_values, network, optimizer, num_steps=50, weights=None):\n",
    "    # train with a sampled dataset\n",
    "    target_qvalues = torch.tensor(target_values, dtype=torch.float32)\n",
    "    s = torch.tensor(s, dtype=torch.int64)\n",
    "    a = torch.tensor(a, dtype=torch.int64)\n",
    "    pred_qvalues = network(s)\n",
    "    pred_qvalues = pred_qvalues.gather(1, a.reshape(-1,1)).squeeze()\n",
    "    loss = torch.mean((pred_qvalues - target_qvalues)**2)\n",
    "    network.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred_qvalues = network(torch.arange(env.num_states))\n",
    "    return pred_qvalues.detach().numpy()\n",
    "\n",
    "  \n",
    "def fitted_q_iteration(\n",
    "    env, \n",
    "    network,\n",
    "    num_itrs=100, \n",
    "    project_steps=50,\n",
    "    render=False,\n",
    "    weights=None,\n",
    "    sampled=False,\n",
    "    training_dataset=None,\n",
    "    optimal_policy=None,\n",
    "    **kwargs\n",
    "  ):\n",
    "  \"\"\"\n",
    "  Runs Fitted Q-iteration.\n",
    "  \n",
    "  Args:\n",
    "    env: A GridEnv object.\n",
    "    num_itrs (int): Number of FQI iterations to run.\n",
    "    project_steps (int): Number of gradient steps used for projection.\n",
    "    render (bool): If True, will plot q-values after each iteration.\n",
    "    sampled (bool): Whether to use sampled datasets for training or not.\n",
    "    training_dataset (list): list of (s, a, r, ns) pairs\n",
    "  \"\"\"\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "  \n",
    "  optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "  weights_tensor = None\n",
    "  if weights is not None:\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "  \n",
    "  q_values = np.zeros((dS, dA))\n",
    "  kls = []\n",
    "  evals = []\n",
    "  for i in range(num_itrs):\n",
    "    if sampled:\n",
    "      for j in range(project_steps):\n",
    "        training_idx = np.random.choice(np.arange(len(training_dataset)), size=128)\n",
    "        s, a, ns, r = get_tensors(training_dataset, training_idx)\n",
    "        target_values = q_backup_sparse_sampled(env, q_values, s, a, ns, r, **kwargs)\n",
    "        intermed_values = project_qvalues_sampled(\n",
    "            env, s, a, target_values, network, optimizer, weights=None,\n",
    "        )\n",
    "        if j == project_steps - 1:\n",
    "          q_values = intermed_values\n",
    "    else:                          \n",
    "      target_values = q_backup_sparse(env, q_values, **kwargs)\n",
    "      q_values = project_qvalues(target_values, network, optimizer,\n",
    "                                weights=weights_tensor,\n",
    "                                num_steps=project_steps)\n",
    "    \n",
    "    stochastic_policy = compute_policy_stochastic(q_values)\n",
    "    if optimal_policy is not None:\n",
    "      kl = compute_divergence(stochastic_policy, optimal_policy)\n",
    "      kls.append(kl)\n",
    "    policy = compute_policy_deterministic(q_values, eps_greedy=0.1)\n",
    "    ret = compute_return(env,policy, T=num_itrs)\n",
    "    evals.append(ret)\n",
    "    if render:\n",
    "      plot_sa_values(env, q_values, update=True, title='Q-values Iteration %d' %i)\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(kls)\n",
    "  plt.title('KL Divergence from Optimal Policy')\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(evals)\n",
    "  plt.title('Return of Trained Policy')\n",
    "  plt.show()\n",
    "  \n",
    "  return q_values\n",
    "\n",
    "\n",
    "def fitted_q_evaluation(\n",
    "    env, \n",
    "    network,\n",
    "    policy=None,\n",
    "    num_itrs=100, \n",
    "    project_steps=50,\n",
    "    render=False,\n",
    "    weights=None,\n",
    "    sampled=False,\n",
    "    training_dataset=None,\n",
    "    optimal_policy=None,\n",
    "    **kwargs\n",
    "  ):\n",
    "  \"\"\"\n",
    "  Runs Fitted Q-evaluation.\n",
    "  \n",
    "  Args:\n",
    "    env: A GridEnv object.\n",
    "    num_itrs (int): Number of FQI iterations to run.\n",
    "    project_steps (int): Number of gradient steps used for projection.\n",
    "    render (bool): If True, will plot q-values after each iteration.\n",
    "    sampled (bool): Whether to use sampled datasets for training or not.\n",
    "    training_dataset (list): list of (s, a, r, ns) pairs\n",
    "  \"\"\"\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "  \n",
    "  optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "  weights_tensor = None\n",
    "  if weights is not None:\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "    assert weights_tensor.shape == (dS, dA)\n",
    "    \n",
    "  if policy is None:\n",
    "    norm_factor = weights.sum(1).reshape(-1, 1)\n",
    "    norm_factor = np.repeat(norm_factor, dA, axis=1)\n",
    "    policy = weights/norm_factor\n",
    "    assert policy.shape == (dS, dA)\n",
    "    \n",
    "  q_values = np.zeros((dS, dA))\n",
    "  kls = []\n",
    "  kls_target = []\n",
    "  \n",
    "  evals = []\n",
    "  for i in range(num_itrs):\n",
    "    if sampled:\n",
    "      for j in range(project_steps):\n",
    "        training_idx = np.random.choice(np.arange(len(training_dataset)), size=128)\n",
    "        s, a, ns, r = get_tensors(training_dataset, training_idx)\n",
    "        target_values = q_backup_sparse_sampled_policy(env, q_values, s, a, ns, r, policy, **kwargs)\n",
    "        intermed_values = project_qvalues_sampled(\n",
    "            env, s, a, target_values, network, optimizer, weights=None,\n",
    "        )\n",
    "        if j == project_steps - 1:\n",
    "          q_values = intermed_values\n",
    "    else:                          \n",
    "      target_values = q_backup_sparse_policy(env, q_values, policy, **kwargs)\n",
    "      q_values = project_qvalues(target_values, network, optimizer,\n",
    "                                weights=weights_tensor,\n",
    "                                num_steps=project_steps)\n",
    "    \n",
    "    stochastic_policy = compute_policy_stochastic(q_values)\n",
    "    if optimal_policy is not None:\n",
    "      kl = compute_divergence(stochastic_policy, optimal_policy)\n",
    "      kls.append(kl)\n",
    "    \n",
    "    if policy is not None:\n",
    "      kl_targ_policy = compute_divergence(stochastic_policy, policy)\n",
    "      kls_target.append(kl_targ_policy)\n",
    "      \n",
    "    policy = compute_policy_deterministic(q_values, eps_greedy=0.1)\n",
    "    ret = compute_return(env,policy, T=num_itrs)\n",
    "    evals.append(ret)\n",
    "    if render:\n",
    "      plot_sa_values(env, q_values, update=True, title='Q-values Iteration %d' %i)\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(kls)\n",
    "  plt.title('KL Divergence from Optimal Policy')\n",
    "  plt.show()\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(kls_target)\n",
    "  plt.title('KL Divergence from Target Policy')\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(evals)\n",
    "  plt.title('Return of Trained Policy')\n",
    "  plt.show()\n",
    "  \n",
    "  return q_values\n",
    "\n",
    "#@title Conservative Q-Learning\n",
    "\n",
    "def get_fg(transform_type, const_val):\n",
    "    def tanh_transform_fn(q_arr):\n",
    "        return const_val * torch.tanh(q_arr/ (const_val + 1e-9)) \n",
    "    \n",
    "    def exp_transform_fn(q_arr):\n",
    "        return torch.exp(q_arr/ (const_val + 1e-9)) \n",
    "\n",
    "    def cent_xexp_transform_fn(q_arr):\n",
    "        cent = q_arr.mean().detach()\n",
    "        q_cent = (q_arr - cent) / (const_val + 1e-9)\n",
    "        return  q_cent * torch.exp(q_cent)\n",
    "    \n",
    "    def norm_xexp_transform_fn(q_arr, return_weight=False):\n",
    "        q_new = (q_arr/(const_val + 1e-9))\n",
    "        q_new = q_new - q_new.max()\n",
    "        weight = torch.exp(q_new)/(torch.exp(q_new).sum() + 1e-9)\n",
    "        weight = weight.detach()\n",
    "        \n",
    "        if return_weight:\n",
    "            return weight\n",
    "        return  q_arr * weight\n",
    "\n",
    "    def norm_neg_xexp_transform_fn(q_arr, return_weight=False):\n",
    "        q_new = -(q_arr/(const_val + 1e-9))\n",
    "        q_new = q_new - q_new.max()\n",
    "        weight = torch.exp(q_new)/(torch.exp(q_new).sum() + 1e-9)\n",
    "        weight = weight.detach()\n",
    "        \n",
    "        if return_weight:\n",
    "            return weight\n",
    "        return  q_arr * weight\n",
    "    \n",
    "    def neg_exp_transform_fn(q_arr):\n",
    "        return const_val * -1 * torch.exp(-1*q_arr/ (const_val + 1e-9)) \n",
    "    \n",
    "    def log_transform_fun(q_arr):\n",
    "        return const_val * torch.log(q_arr/ (const_val + 1e-9)) \n",
    "\n",
    "    def sig_transform_fun(q_arr):\n",
    "        return const_val * torch.sigmoid(q_arr/ (const_val + 1e-9)) \n",
    "    \n",
    "    def identity(q_arr):\n",
    "        return q_arr\n",
    "    \n",
    "    f,g = identity, identity\n",
    "    if transform_type > 0:\n",
    "        if transform_type == 1:\n",
    "            f = g = neg_exp_transform_fn\n",
    "        elif transform_type == 2:\n",
    "            f = neg_exp_transform_fn\n",
    "            g = identity\n",
    "        elif transform_type == 3:\n",
    "            f = g = log_transform_fun\n",
    "        elif transform_type == 4:\n",
    "            f = g = sig_transform_fun\n",
    "        elif transform_type == 5:\n",
    "            f = identity\n",
    "            g = exp_transform_fn\n",
    "        elif transform_type == 6:\n",
    "            f = identity\n",
    "            g = cent_xexp_transform_fn\n",
    "        elif transform_type == 7:\n",
    "            f = identity\n",
    "            g = norm_xexp_transform_fn\n",
    "        elif transform_type == 8:\n",
    "            f = norm_xexp_transform_fn\n",
    "            g = norm_xexp_transform_fn\n",
    "        elif transform_type == 9:\n",
    "            f = norm_neg_xexp_transform_fn\n",
    "            g = norm_neg_xexp_transform_fn\n",
    "        elif transform_type == 10:\n",
    "            f = norm_xexp_transform_fn\n",
    "            g = identity\n",
    "        elif transform_type == 11:\n",
    "            f = g = tanh_transform_fn\n",
    "        elif transform_type == 12:\n",
    "            f = g = exp_transform_fn\n",
    "        else:\n",
    "            assert False\n",
    "    return f,g\n",
    "\n",
    "def project_qvalues_cql(q_values, network, optimizer, num_steps=50, cql_alpha=0.1, weights=None, transform_type=0, const_transform=2):\n",
    "    # regress onto q_values (aka projection)\n",
    "    q_values_tensor = torch.tensor(q_values, dtype=torch.float32)\n",
    "    for _ in range(num_steps):\n",
    "       # Eval the network at each state\n",
    "      pred_qvalues = network(torch.arange(q_values.shape[0]))\n",
    "      if weights is None:\n",
    "        loss = torch.mean((pred_qvalues - q_values_tensor)**2)\n",
    "      else:\n",
    "        loss = torch.mean(weights*(pred_qvalues - q_values_tensor)**2)\n",
    "\n",
    "      # Add cql_loss\n",
    "      # You can have two variants of this loss, one where data q-values\n",
    "      # also maximized (CQL-v2), and one where only the large Q-values \n",
    "      # are pushed down (CQL-v1) as covered in the tutorial\n",
    "      f,g = get_fg(transform_type, const_transform)\n",
    "      cql_loss = torch.logsumexp(f(pred_qvalues), dim=-1, keepdim=True) - g(pred_qvalues)\n",
    "      loss = loss + cql_alpha * torch.mean(weights * cql_loss)\n",
    "      network.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    return pred_qvalues.detach().numpy()\n",
    "\n",
    "def project_qvalues_cql_sampled(env, s, a, target_values, network, optimizer, cql_alpha=0.1, num_steps=50, weights=None, transform_type=0, const_transform=2, mc_returns = None):\n",
    "    # train with a sampled dataset\n",
    "    target_qvalues = torch.tensor(target_values, dtype=torch.float32)\n",
    "    s = torch.tensor(s, dtype=torch.int64)\n",
    "    a = torch.tensor(a, dtype=torch.int64)    \n",
    "    pred_qvalues = network(s)\n",
    "    \n",
    "    if mc_returns is None:\n",
    "      pre_lse_q_values = pred_qvalues\n",
    "    else:\n",
    "      # mc return is |s| x |a|\n",
    "      mc_returns = torch.from_numpy(mc_returns)\n",
    "      assert s.shape == a.shape, f\"{s.shape} != {a.shape}\"\n",
    "      mc_returns = mc_returns[s]\n",
    "      assert mc_returns.shape == pred_qvalues.shape, f\"{mc_returns.shape} != {pred_qvalues.shape}\"\n",
    "      mc_returns = mc_returns.detach()\n",
    "      \n",
    "      pre_lse_q_values = torch.maximum(pred_qvalues, mc_returns)\n",
    "\n",
    "    # f,g = get_fg(transform_type, const_transform)\n",
    "    identity = lambda x: x\n",
    "    f, g = identity, identity\n",
    "    \n",
    "    ### SZ: 1.3 put back the MC here\n",
    "    \n",
    "    logsumexp_qvalues = torch.logsumexp(f(pre_lse_q_values), dim=-1)\n",
    "    \n",
    "    pred_qvalues = pred_qvalues.gather(1, a.reshape(-1,1)).squeeze()\n",
    "    cql_loss = logsumexp_qvalues - g(pred_qvalues)\n",
    "    \n",
    "    loss = torch.mean((pred_qvalues - target_qvalues)**2)\n",
    "    loss = loss + cql_alpha * torch.mean(cql_loss)\n",
    "\n",
    "    network.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred_qvalues = network(torch.arange(env.num_states))\n",
    "    return pred_qvalues.detach().numpy()\n",
    "  \n",
    "def conservative_q_iteration(env, \n",
    "                             network,\n",
    "                             num_itrs=100, \n",
    "                             project_steps=50,\n",
    "                             cql_alpha=0.1,\n",
    "                             render=False,\n",
    "                             weights=None,\n",
    "                             sampled=False,\n",
    "                             transform_type=0,\n",
    "                             const_transform=2,\n",
    "                             training_dataset=None,\n",
    "                             optimal_policy=None,\n",
    "                             mc_returns=None,\n",
    "                             **kwargs):\n",
    "  \"\"\"\n",
    "  Runs Conservative Q-iteration.\n",
    "  \n",
    "  Args:\n",
    "    env: A GridEnv object.\n",
    "    num_itrs (int): Number of FQI iterations to run.\n",
    "    project_steps (int): Number of gradient steps used for projection.\n",
    "    cql_alpha (float): Value of weight on the CQL coefficient.\n",
    "    render (bool): If True, will plot q-values after each iteration.\n",
    "    sampled (bool): Whether to use sampled datasets for training or not.\n",
    "    training_dataset (list): list of (s, a, r, ns) pairs\n",
    "  \"\"\"\n",
    "  dS = env.num_states\n",
    "  dA = env.num_actions\n",
    "  \n",
    "  optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "  weights_tensor = None\n",
    "  if weights is not None:\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "  \n",
    "  q_values = np.zeros((dS, dA))\n",
    "  assert mc_returns is None or mc_returns.shape == q_values.shape, f\"mc_returns shape {mc_returns.shape} does not match q_values shape {q_values.shape}\"\n",
    "  \n",
    "  kls = []\n",
    "  evals = []\n",
    "  for i in range(num_itrs):\n",
    "    if sampled:\n",
    "      for j in range(project_steps):\n",
    "        training_idx = np.random.choice(np.arange(len(training_dataset)), size=128)\n",
    "        s, a, ns, r = get_tensors(training_dataset, training_idx)\n",
    "        target_values = q_backup_sparse_sampled(env, q_values, s, a, ns, r, **kwargs)\n",
    "        \n",
    "        intermed_values = project_qvalues_cql_sampled(\n",
    "            env, s, a, target_values, network, optimizer, \n",
    "            cql_alpha=cql_alpha, weights=None, \n",
    "            transform_type=transform_type, const_transform=const_transform,\n",
    "            mc_returns=mc_returns,\n",
    "        )\n",
    "        if j == project_steps - 1:\n",
    "          q_values = intermed_values\n",
    "    else:\n",
    "      target_values = q_backup_sparse(env, q_values, **kwargs)\n",
    "      q_values = project_qvalues_cql(target_values, network, optimizer,\n",
    "                                weights=weights_tensor,\n",
    "                                cql_alpha=cql_alpha,\n",
    "                                num_steps=project_steps,\n",
    "                                transform_type=transform_type, const_transform=const_transform)\n",
    "    ### SZ: edited Dec 31: added mc_returns\n",
    "    ### SZ: editied Jan 01: moved mc_returns to project_qvalues_cql_sampled\n",
    "    stochastic_policy = compute_policy_stochastic(q_values)\n",
    "    if optimal_policy is not None:\n",
    "      kl = compute_divergence(stochastic_policy, optimal_policy)\n",
    "      kls.append(kl)\n",
    "    policy = compute_policy_deterministic(q_values, eps_greedy=0.1)\n",
    "    ret = compute_return(env,policy, T=num_itrs)\n",
    "    evals.append(ret)\n",
    "    if render:\n",
    "      plot_sa_values(env, q_values, update=True, title='Q-values Iteration %d' %i)\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(kls)\n",
    "  plt.title('KL Divergence from Optimal Policy')\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(evals)\n",
    "  plt.title('Return of Trained Policy')\n",
    "  plt.show()\n",
    "  return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 981
    },
    "id": "YL3SSM-A0f3O",
    "outputId": "a4187ada-d7be-4da0-be69-d71349a6acb7"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Setup Block # 3:\n",
    "    SZ: edited Dec. 29, re-arranged\n",
    "    This block contains setup for parameters\n",
    "\"\"\"\n",
    "\n",
    "# @title Input Parameters\n",
    "\n",
    "weighting_only = True #@param {type: \"boolean\"}\n",
    "dataset_composition = 'random+optimal' #@param [\"optimal\", \"random\", \"random+optimal\", \"mixed\", 'mixed_limited', 'mixed_limited_skewed']\n",
    "dataset_size =  5000#@param {type: \"integer\"}\n",
    "env_type = 'smooth' #@param [\"smooth\", \"random\", \"onehot\"]\n",
    "\n",
    "maze_type = 0\n",
    "\n",
    "def valid_fn_default(x,y):\n",
    "    return True # default valid function\n",
    "\n",
    "def valid_fn_staged(x,y):\n",
    "    return 0 <= x <= 3 or 7 <= x <= 11 or 15 <= x <= 19\n",
    "\n",
    "valid_fn = valid_fn_default\n",
    "if maze_type == 0:\n",
    "    maze_str = (\n",
    "        \"SO#OOOOO\\\\\"+\n",
    "        \"O#OO###O\\\\\"+\n",
    "        \"O#O##ROO\\\\\"+\n",
    "        \"OOOO##O#\\\\\"\n",
    "    )\n",
    "elif maze_type == 1:\n",
    "    maze_str = (\n",
    "        \"SOOOLLLL\\\\\"+\n",
    "        \"OOOOLOOO\\\\\"+\n",
    "        \"OOOOOOLO\\\\\"+ \n",
    "        \"OOOOLLLR\\\\\"\n",
    "    )\n",
    "elif maze_type == 2:\n",
    "    maze_str = (\n",
    "        \"SOOOLLLLOOOOLLLLOOOOLLLL\\\\\"+\n",
    "        \"OOOOLOOOOOOOLOOOOOOOLOOO\\\\\"+\n",
    "        \"OOOOOOLOOOOOOOLOOOOOOOLO\\\\\"+\n",
    "        \"OOOOLLLOOOOOLLLOOOOOLLLR\\\\\"\n",
    "    )\n",
    "    valid_fn = valid_fn_staged \n",
    "elif maze_type == 3:\n",
    "    maze_str = (\n",
    "        \"SOOO####OOOO####OOOO####\\\\\"+\n",
    "        \"OOOO#OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO###OOOOO###OOOOO###R\\\\\"\n",
    "    )\n",
    "    valid_fn = valid_fn_staged\n",
    "elif maze_type == 4:\n",
    "    maze_str = (\n",
    "        \"########################\\\\\"+\n",
    "        \"SOOO####OOOO####OOOO####\\\\\"+\n",
    "        \"OOOO#OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO###OOOOO###OOOOO###O\\\\\"+\n",
    "        \"#######################O\\\\\"+\n",
    "        \"#OOR####OOOO####OOOO###O\\\\\"+\n",
    "        \"#O###OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"#OOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"#######OOOOO###OOOOO###O\\\\\"\n",
    "    )\n",
    "    valid_fn = valid_fn_staged\n",
    "elif maze_type == 5:\n",
    "    maze_str = (\n",
    "        \"########################\\\\\"+\n",
    "        \"SOOO####OOOO####OOOO####\\\\\"+\n",
    "        \"OOOO#OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO###OOOOO###OOOOO###O\\\\\"+\n",
    "        \"#######################O\\\\\"+\n",
    "        \"OOOO####OOOO####OOOO###O\\\\\"+\n",
    "        \"OOOO#OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO###OOOOO###OOOOO###O\\\\\"+\n",
    "        \"OOOO####################\\\\\"+\n",
    "        \"OOOO####OOOO####OOOO###O\\\\\"+\n",
    "        \"OOOO#OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO###OOOOO###OOOOO###R\\\\\"+\n",
    "        \"########################\\\\\"\n",
    "    )\n",
    "    valid_fn = valid_fn_staged\n",
    "elif maze_type == 6:\n",
    "    maze_str = (\n",
    "        \"########################\\\\\"+\n",
    "        \"SOOO#OOOOOOO####OOOO####\\\\\"+\n",
    "        \"OOOO#O#OOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO##OOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO#OO#OOOO###OOOOO###O\\\\\"+\n",
    "        \"#######################O\\\\\"+\n",
    "        \"OOOO#OOO#OOO####OOOO###O\\\\\"+\n",
    "        \"OOOOOO#O#OOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOO#O#O#OOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO#O#OOOOO###OOOOO###O\\\\\"+\n",
    "        \"OOOO####################\\\\\"+\n",
    "        \"OOOO####OOOO####OOOO###O\\\\\"+\n",
    "        \"OOOO#OOOOOOO#OOOOOOO#OOO\\\\\"+\n",
    "        \"OOOOOO#OOOOOOO#OOOOOOO#O\\\\\"+\n",
    "        \"OOOO#O#OOOOO###OOOOO###R\\\\\"+\n",
    "        \"########################\\\\\"\n",
    "    )\n",
    "    valid_fn = valid_fn_staged\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "maze = spec_from_string(maze_str)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(maze._GridSpec__data, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "env = GridEnv(maze, observation_type=env_type, dim_obs=8)\n",
    "\n",
    "if maze_type in list(range(0,4)):\n",
    "    num_itrs=1000\n",
    "    discount=0.95\n",
    "elif maze_type in list(range(4,6)):    \n",
    "    num_itrs=1000\n",
    "    discount=0.98\n",
    "else:\n",
    "    num_itrs=1000\n",
    "    discount=0.99\n",
    "print('iters', num_itrs)\n",
    "print('discount', discount)\n",
    "\n",
    "optimal_qvalues = q_iteration(env, num_itrs=num_itrs, discount=discount, render=False)\n",
    "\n",
    "plot_sa_values(env, optimal_qvalues, title='Q*-values')\n",
    "\n",
    "optimal_policy = policy = compute_policy_deterministic(optimal_qvalues, eps_greedy=0.1)\n",
    "sa_visitations = compute_visitation(env, policy, T=num_itrs)\n",
    "plot_sa_values(env, sa_visitations, title='Optimal policy state-action visitation')\n",
    "\n",
    "compute_return(env, optimal_policy, T=num_itrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHDS0_u90x15"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Setup Block # 4:\n",
    "    SZ: edited Dec. 29, re-arranged\n",
    "    This block contains setup for weights\n",
    "\"\"\"\n",
    "\n",
    "#@title Compute weights\n",
    "if dataset_composition == 'optimal':\n",
    "  \"\"\"Distribution of the optimal policy (+ some noise)\"\"\"\n",
    "  weights = sa_visitations\n",
    "  weights = weights/ np.sum(weights)\n",
    "elif dataset_composition == 'random':\n",
    "  \"\"\"A random disribution over states and actions\"\"\" \n",
    "  weights = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "  weights = np.reshape(weights, (env.num_states, env.num_actions))\n",
    "  weights = weights/ np.sum(weights)\n",
    "elif dataset_composition == 'random+optimal':\n",
    "  \"\"\"Mixture of random and optimal policies\"\"\"\n",
    "  weights = sa_visitations / np.sum(sa_visitations)\n",
    "  weights_rand = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "  weights_rand = np.reshape(weights_rand, (env.num_states, env.num_actions)) / np.sum(weights_rand)\n",
    "  weights = (weights_rand + weights)/2.0\n",
    "elif dataset_composition == 'mixed_limited':\n",
    "  weights = sa_visitations / np.sum(sa_visitations)\n",
    "  weights_rand = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "  weights_rand = np.reshape(weights_rand, (env.num_states, env.num_actions)) / np.sum(weights_rand)\n",
    "  \n",
    "  w = env.gs.width\n",
    "  h = env.gs.height\n",
    "  for x, y in itertools.product(range(w), range(h)):\n",
    "    state_idx = env.gs.xy_to_idx((x, y))\n",
    "    weights[state_idx] = weights_rand[state_idx] if valid_fn(x,y) else (weights_rand[state_idx] + weights[state_idx])/2\n",
    "  weights = weights/np.sum(weights)\n",
    "elif dataset_composition == 'mixed_limited_skewed':\n",
    "  skew_weight = 10\n",
    "  weights = sa_visitations / np.sum(sa_visitations)\n",
    "  weights_rand = np.random.uniform(size=(env.num_states, env.num_actions))\n",
    "  weights_rand[:,3] *= skew_weight\n",
    "  weights_rand = weights_rand/np.sum(weights_rand.flatten())\n",
    "  \n",
    "  w = env.gs.width\n",
    "  h = env.gs.height\n",
    "  for x, y in itertools.product(range(w), range(h)):\n",
    "    state_idx = env.gs.xy_to_idx((x, y))\n",
    "    weights[state_idx] = weights_rand[state_idx] if valid_fn(x,y) else weights[state_idx]\n",
    "  weights = weights/np.sum(weights)\n",
    "elif dataset_composition == 'mixed':\n",
    "  \"\"\"Mixture of policies corresponding to random Q-values\"\"\"\n",
    "  num_policies_mix = 4\n",
    "  weights = np.zeros_like(sa_visitations)\n",
    "  for idx in range(num_policies_mix):\n",
    "    rand_q_vals_idx = np.random.uniform(low=0.0, high=10.0, size=(env.num_states, env.num_actions))\n",
    "    policy_idx = compute_policy_deterministic(rand_q_vals_idx, eps_greedy=0.1)\n",
    "    sa_visitations_idx = compute_visitation(env, policy_idx)\n",
    "    weights = weights + sa_visitations_idx\n",
    "  weights = weights / np.sum(weights)\n",
    "\n",
    "# @title Generate dataset\n",
    "weighting_only=False\n",
    "if not weighting_only:\n",
    "  weights_flatten = np.reshape(weights, -1)\n",
    "  weights_flatten = weights_flatten/ np.sum(weights_flatten)\n",
    "  dataset = np.random.choice(\n",
    "      np.arange(env.num_states * env.num_actions),\n",
    "      size=dataset_size, replace=True, p=weights_flatten\n",
    "  )\n",
    "  training_sa_pairs = [(int(val//env.num_actions), val % env.num_actions) for val in dataset]\n",
    "\n",
    "  # Now sample (s', r) values for training as well\n",
    "  training_dataset = []\n",
    "  training_data_dist = np.zeros((env.num_states, env.num_actions))\n",
    "  for idx in range(len(training_sa_pairs)):\n",
    "    s, a = training_sa_pairs[idx]\n",
    "    prob_s_prime = env._transition_matrix[s, a]\n",
    "    s_prime = np.random.choice(np.arange(env.num_states), p=prob_s_prime)\n",
    "    r = env.reward(s, a, s_prime)\n",
    "    training_dataset.append((s, a, r, s_prime))\n",
    "    training_data_dist[s, a] += 1.0\n",
    "else:\n",
    "  # Using only weighting style dataset\n",
    "  training_dataset = None\n",
    "  training_data_dist = None\n",
    "\n",
    "#@title Visualize dataset or weights\n",
    "plot_sa_values(env, training_data_dist, title='Dataset composition')\n",
    "plot_sa_values(env, weights, title='Weighting Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test rollout function\n",
    "dS = env.num_states\n",
    "dA = env.num_actions\n",
    "rollouts_optimal = rollout(env, optimal_policy, T=50, render=True)\n",
    "\n",
    "test_q_function = torch.randn(dS, dA)\n",
    "policy = compute_policy_deterministic(test_q_function, eps_greedy=0)\n",
    "policy_stoch = compute_policy_stochastic(test_q_function)\n",
    "rollouts_policy = rollout(env, policy, T=50, render=True)\n",
    "\n",
    "def plot_fig_time(arr, name):\n",
    "    plt.figure()\n",
    "    plt.plot(arr)\n",
    "    plt.title(name)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(name)\n",
    "    plt.show()\n",
    "\n",
    "s_opt = [x[0] for x in rollouts_optimal]\n",
    "a_opt = [x[1] for x in rollouts_optimal]\n",
    "r_opt = [x[2] for x in rollouts_optimal]\n",
    "ns_opt = [x[3] for x in rollouts_optimal]\n",
    "cum_r_opt = np.cumsum(r_opt)\n",
    "\n",
    "plot_fig_time(s_opt, 'Optimal state')\n",
    "plot_fig_time(a_opt, 'Optimal action')\n",
    "plot_fig_time(r_opt, 'Optimal reward')\n",
    "plot_fig_time(ns_opt, 'Optimal next state')\n",
    "plot_fig_time(cum_r_opt, 'Optimal cumulative reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(res, dataset_scale):\n",
    "    ### Generating Dataset\n",
    "    dataset_size = (res+1) * dataset_scale\n",
    "    \n",
    "    #@title Compute weights\n",
    "    if dataset_composition == 'optimal':\n",
    "      \"\"\"Distribution of the optimal policy (+ some noise)\"\"\"\n",
    "      weights = sa_visitations\n",
    "      weights = weights/ np.sum(weights)\n",
    "    elif dataset_composition == 'random':\n",
    "      \"\"\"A random disribution over states and actions\"\"\" \n",
    "      weights = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "      weights = np.reshape(weights, (env.num_states, env.num_actions))\n",
    "      weights = weights/ np.sum(weights)\n",
    "    elif dataset_composition == 'random+optimal':\n",
    "      \"\"\"Mixture of random and optimal policies\"\"\"\n",
    "      weights = sa_visitations / np.sum(sa_visitations)\n",
    "      weights_rand = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "      weights_rand = np.reshape(weights_rand, (env.num_states, env.num_actions)) / np.sum(weights_rand)\n",
    "      weights = (weights_rand + weights)/2.0\n",
    "    elif dataset_composition == 'mixed_limited':\n",
    "      weights = sa_visitations / np.sum(sa_visitations)\n",
    "      weights_rand = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "      weights_rand = np.reshape(weights_rand, (env.num_states, env.num_actions)) / np.sum(weights_rand)\n",
    "\n",
    "      w = env.gs.width\n",
    "      h = env.gs.height\n",
    "      for x, y in itertools.product(range(w), range(h)):\n",
    "        state_idx = env.gs.xy_to_idx((x, y))\n",
    "        weights[state_idx] = weights_rand[state_idx] if valid_fn(x,y) else (weights_rand[state_idx] + weights[state_idx])/2\n",
    "      weights = weights/np.sum(weights)\n",
    "    elif dataset_composition == 'mixed_limited_skewed':\n",
    "      skew_weight = 10\n",
    "      weights = sa_visitations / np.sum(sa_visitations)\n",
    "      weights_rand = np.random.uniform(size=(env.num_states, env.num_actions))\n",
    "      weights_rand[:,3] *= skew_weight\n",
    "      weights_rand = weights_rand/np.sum(weights_rand.flatten())\n",
    "\n",
    "      w = env.gs.width\n",
    "      h = env.gs.height\n",
    "      for x, y in itertools.product(range(w), range(h)):\n",
    "        state_idx = env.gs.xy_to_idx((x, y))\n",
    "        weights[state_idx] = weights_rand[state_idx] if valid_fn(x,y) else weights[state_idx]\n",
    "      weights = weights/np.sum(weights)\n",
    "    elif dataset_composition == 'mixed':\n",
    "      \"\"\"Mixture of policies corresponding to random Q-values\"\"\"\n",
    "      num_policies_mix = 4\n",
    "      weights = np.zeros_like(sa_visitations)\n",
    "      for idx in range(num_policies_mix):\n",
    "        rand_q_vals_idx = np.random.uniform(low=0.0, high=10.0, size=(env.num_states, env.num_actions))\n",
    "        policy_idx = compute_policy_deterministic(rand_q_vals_idx, eps_greedy=0.1)\n",
    "        sa_visitations_idx = compute_visitation(env, policy_idx)\n",
    "        weights = weights + sa_visitations_idx\n",
    "      weights = weights / np.sum(weights)\n",
    "\n",
    "    # @title Generate dataset\n",
    "    weighting_only=False\n",
    "    if not weighting_only:\n",
    "      weights_flatten = np.reshape(weights, -1)\n",
    "      weights_flatten = weights_flatten/ np.sum(weights_flatten)\n",
    "      dataset = np.random.choice(\n",
    "          np.arange(env.num_states * env.num_actions),\n",
    "          size=dataset_size, replace=True, p=weights_flatten\n",
    "      )\n",
    "      training_sa_pairs = [(int(val//env.num_actions), val % env.num_actions) for val in dataset]\n",
    "\n",
    "      # Now sample (s', r) values for training as well\n",
    "      training_dataset = []\n",
    "      training_data_dist = np.zeros((env.num_states, env.num_actions))\n",
    "      for idx in range(len(training_sa_pairs)):\n",
    "        s, a = training_sa_pairs[idx]\n",
    "        prob_s_prime = env._transition_matrix[s, a]\n",
    "        s_prime = np.random.choice(np.arange(env.num_states), p=prob_s_prime)\n",
    "        r = env.reward(s, a, s_prime)\n",
    "        training_dataset.append((s, a, r, s_prime))\n",
    "        training_data_dist[s, a] += 1.0\n",
    "    else:\n",
    "      # Using only weighting style dataset\n",
    "      training_dataset = None\n",
    "      training_data_dist = None\n",
    "    \n",
    "    return training_dataset, training_data_dist, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "### Basic params for dataset \n",
    "dataset_composition = 'random+optimal' #@param [\"optimal\", \"random\", \"random+optimal\", \"mixed\", 'mixed_limited', 'mixed_limited_skewed']\n",
    "env_type = 'smooth' #@param [\"smooth\", \"random\", \"onehot\"]\n",
    "\n",
    "\n",
    "### Total number of independent run\n",
    "num_seed = 1\n",
    "\n",
    "### Length of x axis (number of different samples to be tested)\n",
    "num_resolution = 2\n",
    "dataset_scale = 3000\n",
    "num_online_resolutions = 2\n",
    "online_dataset_scale = 1000\n",
    "utd=1\n",
    "\n",
    "### Setting CQL params\n",
    "cql_alpha_val = 0.1 # @param {type:\"slider\", min:0.0, max:10.0, step:0.1}\n",
    "transform_type = 7 # @param\n",
    "const_transform = 1 # @param\n",
    "\n",
    "\n",
    "### Initializing two arrays for storing the final values\n",
    "vanilla_cql_return = np.zeros(num_resolution)\n",
    "mc_sampled_cql_return = np.zeros(num_resolution)\n",
    "mc_true_cql_return = np.zeros(num_resolution)\n",
    "\n",
    "vanilla_cql_return_online = np.zeros((num_resolution, num_online_resolutions))\n",
    "mc_sampled_cql_return_online = np.zeros((num_resolution, num_online_resolutions))\n",
    "mc_true_cql_return_online = np.zeros((num_resolution, num_online_resolutions))\n",
    "online_returns=dict(\n",
    "  vanilla_cql=vanilla_cql_return_online,\n",
    "  cql_sampledmc=mc_sampled_cql_return_online,\n",
    "  cql_truemc=mc_true_cql_return_online,\n",
    ")\n",
    "\n",
    "from typing import List, Tuple\n",
    "class TrainState:\n",
    "  network: torch.nn.Module\n",
    "  q_values: np.ndarray   \n",
    "  policy: np.ndarray\n",
    "  dataset: List[Tuple[int, int, float, int]]\n",
    "  num_itrs: int = 0\n",
    "  kwargs: dict = {}\n",
    "\n",
    "for num in range(num_seed):\n",
    "  for res in range(num_resolution):\n",
    "    \n",
    "    ### Generating Dataset\n",
    "    dataset_size = (res+1) * dataset_scale\n",
    "    \n",
    "    #@title Compute weights\n",
    "    if dataset_composition == 'optimal':\n",
    "      \"\"\"Distribution of the optimal policy (+ some noise)\"\"\"\n",
    "      weights = sa_visitations\n",
    "      weights = weights/ np.sum(weights)\n",
    "    elif dataset_composition == 'random':\n",
    "      \"\"\"A random disribution over states and actions\"\"\" \n",
    "      weights = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "      weights = np.reshape(weights, (env.num_states, env.num_actions))\n",
    "      weights = weights/ np.sum(weights)\n",
    "    elif dataset_composition == 'random+optimal':\n",
    "      \"\"\"Mixture of random and optimal policies\"\"\"\n",
    "      weights = sa_visitations / np.sum(sa_visitations)\n",
    "      weights_rand = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "      weights_rand = np.reshape(weights_rand, (env.num_states, env.num_actions)) / np.sum(weights_rand)\n",
    "      weights = (weights_rand + weights)/2.0\n",
    "    elif dataset_composition == 'mixed_limited':\n",
    "      weights = sa_visitations / np.sum(sa_visitations)\n",
    "      weights_rand = np.random.uniform(size=env.num_states * env.num_actions)\n",
    "      weights_rand = np.reshape(weights_rand, (env.num_states, env.num_actions)) / np.sum(weights_rand)\n",
    "\n",
    "      w = env.gs.width\n",
    "      h = env.gs.height\n",
    "      for x, y in itertools.product(range(w), range(h)):\n",
    "        state_idx = env.gs.xy_to_idx((x, y))\n",
    "        weights[state_idx] = weights_rand[state_idx] if valid_fn(x,y) else (weights_rand[state_idx] + weights[state_idx])/2\n",
    "      weights = weights/np.sum(weights)\n",
    "    elif dataset_composition == 'mixed_limited_skewed':\n",
    "      skew_weight = 10\n",
    "      weights = sa_visitations / np.sum(sa_visitations)\n",
    "      weights_rand = np.random.uniform(size=(env.num_states, env.num_actions))\n",
    "      weights_rand[:,3] *= skew_weight\n",
    "      weights_rand = weights_rand/np.sum(weights_rand.flatten())\n",
    "\n",
    "      w = env.gs.width\n",
    "      h = env.gs.height\n",
    "      for x, y in itertools.product(range(w), range(h)):\n",
    "        state_idx = env.gs.xy_to_idx((x, y))\n",
    "        weights[state_idx] = weights_rand[state_idx] if valid_fn(x,y) else weights[state_idx]\n",
    "      weights = weights/np.sum(weights)\n",
    "    elif dataset_composition == 'mixed':\n",
    "      \"\"\"Mixture of policies corresponding to random Q-values\"\"\"\n",
    "      num_policies_mix = 4\n",
    "      weights = np.zeros_like(sa_visitations)\n",
    "      for idx in range(num_policies_mix):\n",
    "        rand_q_vals_idx = np.random.uniform(low=0.0, high=10.0, size=(env.num_states, env.num_actions))\n",
    "        policy_idx = compute_policy_deterministic(rand_q_vals_idx, eps_greedy=0.1)\n",
    "        sa_visitations_idx = compute_visitation(env, policy_idx)\n",
    "        weights = weights + sa_visitations_idx\n",
    "      weights = weights / np.sum(weights)\n",
    "\n",
    "    # @title Generate dataset\n",
    "    weighting_only=False\n",
    "    if not weighting_only:\n",
    "      weights_flatten = np.reshape(weights, -1)\n",
    "      weights_flatten = weights_flatten/ np.sum(weights_flatten)\n",
    "      dataset = np.random.choice(\n",
    "          np.arange(env.num_states * env.num_actions),\n",
    "          size=dataset_size, replace=True, p=weights_flatten\n",
    "      )\n",
    "      training_sa_pairs = [(int(val//env.num_actions), val % env.num_actions) for val in dataset]\n",
    "\n",
    "      # Now sample (s', r) values for training as well\n",
    "      training_dataset = []\n",
    "      training_data_dist = np.zeros((env.num_states, env.num_actions))\n",
    "      for idx in range(len(training_sa_pairs)):\n",
    "        s, a = training_sa_pairs[idx]\n",
    "        prob_s_prime = env._transition_matrix[s, a]\n",
    "        s_prime = np.random.choice(np.arange(env.num_states), p=prob_s_prime)\n",
    "        r = env.reward(s, a, s_prime)\n",
    "        training_dataset.append((s, a, r, s_prime))\n",
    "        training_data_dist[s, a] += 1.0\n",
    "    else:\n",
    "      # Using only weighting style dataset\n",
    "      training_dataset = None\n",
    "      training_data_dist = None\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute Block # 1\n",
    "    \"\"\"\n",
    "    # Use a tabular or feedforward NN approximator\n",
    "    init_network = lambda : FCNetwork(env, layers=[20, 20])\n",
    "    network_fqe = init_network()\n",
    "    network_cql = init_network()\n",
    "    network_cql_mcsampled = init_network()\n",
    "\n",
    "    # Run Q-iteration\n",
    "    print(\" === computing mc returns from sampled data === \")\n",
    "    q_mc_returns_sampled = fitted_q_evaluation(\n",
    "      env, \n",
    "      network_fqe,\n",
    "      num_itrs=num_itrs, \n",
    "      discount=discount, \n",
    "      weights=weights, \n",
    "      render=False,\n",
    "      sampled=True,\n",
    "      training_dataset=training_dataset, \n",
    "      optimal_policy=optimal_policy\n",
    "    )\n",
    "\n",
    "    policy = compute_policy_deterministic(q_mc_returns_sampled, eps_greedy=0) \n",
    "    FQE_return_sampled = compute_return(env, policy, T=num_itrs)\n",
    "    print(f\"FQE_return_sampled: {FQE_return_sampled}\")\n",
    "\n",
    "    if False:\n",
    "      print(\" === computing mc returns from true transition === \")\n",
    "      q_mc_returns_true = fitted_q_evaluation(\n",
    "        env, \n",
    "        network_cql,\n",
    "        num_itrs=num_itrs, \n",
    "        discount=discount, \n",
    "        weights=weights, \n",
    "        render=False,\n",
    "        optimal_policy=optimal_policy\n",
    "      )\n",
    "\n",
    "      policy = compute_policy_deterministic(q_mc_returns_true, eps_greedy=0)\n",
    "      FQE_returns_true = compute_return(env, policy, T=num_itrs)\n",
    "      print(f\"FQE_returns_true: {FQE_returns_true}\")\n",
    "\n",
    "    \"\"\"\n",
    "        Compute Block # 2\n",
    "    \"\"\"\n",
    "\n",
    "    ### Second: Run regular fitted CQL with finite data\n",
    "    \n",
    "    experiments_online=dict()\n",
    "    \n",
    "    ### Run CQL without MC_bound\n",
    "\n",
    "    if False:\n",
    "      print(\" === running vanilla CQL from sampled data === \")\n",
    "\n",
    "      q_values = conservative_q_iteration(\n",
    "        env, \n",
    "        network,\n",
    "        num_itrs=num_itrs, \n",
    "        discount=discount, \n",
    "        cql_alpha=cql_alpha_val, \n",
    "        weights=weights, \n",
    "        render=False,\n",
    "        sampled=True,\n",
    "        training_dataset=training_dataset,\n",
    "        transform_type=transform_type, \n",
    "        const_transform=const_transform,\n",
    "        optimal_policy=optimal_policy\n",
    "      )\n",
    "\n",
    "      #@title Compute visitations of the learned policy\n",
    "      policy = compute_policy_deterministic(q_values, eps_greedy=0)\n",
    "      policy_sa_visitations = compute_visitation(env, policy, T=num_itrs)\n",
    "      plot_sa_values(env, policy, title='pi for CQL (no MC bound)')\n",
    "\n",
    "      cql_return = compute_return(env, policy, T=num_itrs)\n",
    "      vanilla_cql_return[res] += cql_return\n",
    "      \n",
    "      experiments_online['vanilla_cql'] = TrainState(q_values=q_values, policy=policy, dataset=copy.deepcopy(training_dataset))\n",
    "\n",
    "    ### Run CQL with MC_bound + sampled MC value\n",
    "\n",
    "    print(\" === running CQL from sampled data + MC bounds from sampled data === \")\n",
    "\n",
    "    q_values_cql_sampledmc = conservative_q_iteration(env, network_cql_mcsampled,\n",
    "                                        num_itrs=num_itrs, discount=discount, cql_alpha=cql_alpha_val, \n",
    "                                        weights=weights, render=False,\n",
    "                                        sampled=not(weighting_only),\n",
    "                                        training_dataset=training_dataset, \n",
    "                                        transform_type=transform_type, const_transform=const_transform,\n",
    "                                        optimal_policy=optimal_policy, \n",
    "                                        mc_returns=q_mc_returns_sampled)\n",
    "\n",
    "    #@title Compute visitations of the learned policy\n",
    "    policy_sampledmc = compute_policy_deterministic(q_values_cql_sampledmc, eps_greedy=0)\n",
    "    policy_sa_visitations = compute_visitation(env, policy_sampledmc, T=num_itrs)\n",
    "    plot_sa_values(env, policy_sampledmc, title='pi for CQL (MC bound)')\n",
    "\n",
    "    # plot_sa_values(env, policy_sa_visitations, title='Q-hat Visitation')\n",
    "    cql_return_mc_sampled = compute_return(env, policy_sampledmc, T=num_itrs)\n",
    "    mc_sampled_cql_return[res] += cql_return_mc_sampled\n",
    "    \n",
    "    experiments_online['cql_sampledmc'] = TrainState(q_values=q_values_cql_sampledmc, policy=policy_sampledmc, dataset=copy.deepcopy(training_dataset))\n",
    "    \n",
    "    for k, ts in experiments_online.items():\n",
    "      for online_res in range(num_online_resolutions):\n",
    "        while ts.num_itrs < (online_res + 1) * online_dataset_scale:\n",
    "          traj = rollout(env, ts.policy, T=num_itrs, render=False)\n",
    "          ts.dataset.extend(traj)\n",
    "          num_iters_online=len(traj) * utd\n",
    "          ts.num_itrs += num_iters_online\n",
    "          \n",
    "          ts.q_values = conservative_q_iteration(\n",
    "            env,\n",
    "            ts.network,\n",
    "            num_iters=num_iters_online,\n",
    "            discount=discount,\n",
    "            cql_alpha=cql_alpha_val,\n",
    "            weights=weights,\n",
    "            render=False,\n",
    "            sampled=True,\n",
    "            transform_type=transform_type,\n",
    "            const_transform=const_transform,\n",
    "            optimal_policy=optimal_policy,\n",
    "            **ts.kwargs\n",
    "          )\n",
    "          \n",
    "          ts.policy = compute_policy_stochastic(ts.q_values, eps_greedy=0)\n",
    "        policy_det = compute_policy_deterministic(ts.q_values, eps_greedy=0)\n",
    "        ret_det = compute_return(env, policy_det, T=num_itrs)\n",
    "        online_returns[k][res][online_res] = ret_det\n",
    "\n",
    "print(num_online_resolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a line graph\n",
    "plt.plot(vanilla_cql_return/num_seed, label='CQL_return')\n",
    "plt.plot(mc_sampled_cql_return/num_seed, label='CQL_sampled_mc_return')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"CQL with MC lower bounds\")\n",
    "plt.xlabel(\"Sample Complexity  (x+1) * 800\")\n",
    "plt.ylabel(\"Average Return\")\n",
    " \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Offline Rl Gridworld",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8aac0e8baad73e8031a9c7f0c17e1976976027bdaa3789db02891cdc6da540a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
